{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQFWnv0RPAx_",
        "outputId": "d4576c10-a709-4a29-b469-5f075b0481fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "drive_base = '/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq'\n",
        "colab_base = '/content/fashion-iq'\n",
        "\n",
        "# Create destination root in Colab internal storage\n",
        "os.makedirs(colab_base, exist_ok=True)\n",
        "\n",
        "# Define subdirectories\n",
        "subdirs = ['image_splits', 'captions']\n",
        "\n",
        "# Copy 'image_splits' and 'captions' directories\n",
        "for subdir in subdirs:\n",
        "    src = os.path.join(drive_base, subdir)\n",
        "    dst = os.path.join(colab_base, subdir)\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)  # Remove existing to avoid duplication/conflict\n",
        "    shutil.copytree(src, dst)\n",
        "    print(f\"‚úÖ Copied {subdir} to {dst}\")\n",
        "\n",
        "# Handle images.zip\n",
        "zip_path = os.path.join(drive_base, 'images/images.zip')\n",
        "extract_dir = os.path.join(colab_base, 'images')\n",
        "\n",
        "# Make sure destination for images exists\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip images.zip into /content/fashion-iq/images/\n",
        "!unzip -q \"$zip_path\" -d \"$extract_dir\"\n",
        "\n",
        "print(\"‚úÖ Unzipped images.zip into Colab storage.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZEI5zjwQ8jE",
        "outputId": "0689ed3e-f9af-4d3c-a346-4c3af7fc413c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Copied image_splits to /content/fashion-iq/image_splits\n",
            "‚úÖ Copied captions to /content/fashion-iq/captions\n",
            "replace /content/fashion-iq/images/images/245600258X.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: ‚úÖ Unzipped images.zip into Colab storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "drive_base = '/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq'\n",
        "colab_base = '/content/fashion-iq'\n",
        "base_path =  Path(\"/content/fashion-iq\")"
      ],
      "metadata": {
        "id": "3VJUbUdoRLMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCYoUahbRUdq",
        "outputId": "99f84b92-b8b8-4bba-c970-5d819069373e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/image_retrieval')\n",
        "import importlib\n",
        "import data_utils  # initial import\n",
        "from data_utils import targetpad_transform,_convert_image_to_rgb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQfrIYE9RiOT",
        "outputId": "fda8ae61-77e5-4a68-b1dc-a9555670b0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Copied image_splits to /content/fashion-iq/image_splits\n",
            "‚úÖ Copied captions to /content/fashion-iq/captions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95KbjM6wTSq9",
        "outputId": "aa3f8f9e-46ba-4e20-fdf9-bf93bf4df764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-io4dn95y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-io4dn95y\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "9FwarHKkTWPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = targetpad_transform(target_ratio=1.0, dim=224)\n"
      ],
      "metadata": {
        "id": "OHTIkImLT0F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"a clothing item\",\n",
        "    \"a fashion dress\",\n",
        "    \"a shirt or top\",\n",
        "    \"a brand logo\",\n",
        "    \"an image with only text\",\n",
        "    \"an empty product image\",\n",
        "]"
      ],
      "metadata": {
        "id": "2W5H3SknT-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
        "text_tokens = clip.tokenize(prompts).to(device)\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize"
      ],
      "metadata": {
        "id": "bviZ9vG4UB7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "aubFwWgTUj2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = base_path / 'images' / 'images'\n",
        "output_folder = Path(\"/content/fashion-iq-cleaned\")\n",
        "output_folder.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "OnbeyplaUmZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unrelated_images = []\n",
        "for img_path in tqdm(list(image_folder.glob(\"*.png\"))):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_features = model.encode_image(img_tensor)\n",
        "            img_features /= img_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
        "            best_prompt_idx = similarity.argmax().item()\n",
        "            best_prompt = prompts[best_prompt_idx]\n",
        "\n",
        "            if best_prompt in [\"a brand logo\", \"an image with only text\", \"an empty product image\"]:\n",
        "                unrelated_images.append(str(img_path))\n",
        "            else:\n",
        "                shutil.copy(img_path, output_folder / img_path.name)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error reading {img_path.name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-1mA8RJUowh",
        "outputId": "ef884564-f59f-4ad7-d19d-09c9dd3209a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74381/74381 [17:29<00:00, 70.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "cleaned_colab_path = \"/content/fashion-iq-cleaned\"\n",
        "drive_destination = \"/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq-cleaned\"\n",
        "\n",
        "# Remove if already exists in Drive to avoid duplication (optional)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(drive_destination):\n",
        "    shutil.rmtree(drive_destination)\n",
        "\n",
        "# Copy the cleaned folder to your Drive\n",
        "shutil.copytree(cleaned_colab_path, drive_destination)\n",
        "\n",
        "print(\"‚úÖ Cleaned dataset successfully saved to Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcTUYPGKcrQh",
        "outputId": "f15b91d0-f52b-4198-c8cd-b6a93762ad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned dataset successfully saved to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_path = \"/content/fashion-iq-cleaned\"\n",
        "\n",
        "# Count image files (e.g., .png, .jpg, .jpeg)\n",
        "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
        "image_count = sum(\n",
        "    1 for fname in os.listdir(cleaned_path) if fname.lower().endswith(image_extensions)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Number of images in '{cleaned_path}': {image_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XK2UCTCd_D9",
        "outputId": "3593e08c-4074-4678-9861-1508731bf532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Number of images in '/content/fashion-iq-cleaned': 73620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import webcolors\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Install webcolors if not installed\n",
        "!pip install -q webcolors\n",
        "\n",
        "# Helper functions for color processing\n",
        "def extract_dominant_color(image_path, k=4):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize((100, 100))  # Resize for speed\n",
        "        image_np = np.array(image).reshape((-1, 3))\n",
        "\n",
        "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "        kmeans.fit(image_np)\n",
        "\n",
        "        # Find the most common cluster\n",
        "        counts = Counter(kmeans.labels_)\n",
        "        dominant_cluster = counts.most_common(1)[0][0]\n",
        "        dominant_color = kmeans.cluster_centers_[dominant_cluster].astype(int)\n",
        "        return tuple(dominant_color)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "from webcolors import CSS3_HEX_TO_NAMES, hex_to_rgb, rgb_to_name\n",
        "\n",
        "def closest_color(requested_color):\n",
        "    min_colors = {}\n",
        "    for hex_value, name in CSS3_HEX_TO_NAMES.items():\n",
        "        r_c, g_c, b_c = hex_to_rgb(hex_value)\n",
        "        rd = (r_c - requested_color[0]) ** 2\n",
        "        gd = (g_c - requested_color[1]) ** 2\n",
        "        bd = (b_c - requested_color[2]) ** 2\n",
        "        min_colors[(rd + gd + bd)] = name\n",
        "    return min_colors[min(min_colors.keys())]\n",
        "\n",
        "def rgb_to_name(rgb_tuple):\n",
        "    try:\n",
        "        return rgb_to_name(rgb_tuple, spec='css3')\n",
        "    except ValueError:\n",
        "        return closest_color(rgb_tuple)\n",
        "\n",
        "\n",
        "# === Paths and setup ===\n",
        "base_path = Path(\"/content/fashion-iq\")\n",
        "cleaned_images_dir = Path(\"/content/fashion-iq-cleaned\")\n",
        "captions_dir = base_path / \"captions\"\n",
        "output_dir = base_path / \"captions\"/\"captions_with_colors2\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "dress_types = ['dress', 'shirt', 'toptee']\n",
        "splits = ['train', 'val', 'test']  # or just ['train'] if needed\n",
        "\n",
        "# === Loop through all caption files ===\n",
        "for dress_type in dress_types:\n",
        "    for split in splits:\n",
        "        caption_file = captions_dir / f\"cap.{dress_type}.{split}.json\"\n",
        "        if not caption_file.exists():\n",
        "            print(f\"‚ùå Skipping missing file: {caption_file}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"üé® Processing {caption_file}...\")\n",
        "        with open(caption_file, 'r') as f:\n",
        "            triplets = json.load(f)\n",
        "\n",
        "        for triplet in tqdm(triplets):\n",
        "            candidate_path = cleaned_images_dir / f\"{triplet['candidate']}.png\"\n",
        "            target_path = cleaned_images_dir / f\"{triplet['target']}.png\"\n",
        "\n",
        "            candidate_color = extract_dominant_color(candidate_path)\n",
        "            target_color = extract_dominant_color(target_path)\n",
        "\n",
        "            triplet['candidate_color_rgb'] = candidate_color\n",
        "            triplet['candidate_color_name'] = rgb_to_name(candidate_color) if candidate_color else None\n",
        "            triplet['target_color_rgb'] = target_color\n",
        "            triplet['target_color_name'] = rgb_to_name(target_color) if target_color else None\n",
        "\n",
        "        # Save updated JSON\n",
        "        out_file = output_dir / f\"cap.{dress_type}.{split}.json\"\n",
        "        with open(out_file, 'w') as f:\n",
        "            json.dump(triplets, f, indent=2)\n",
        "        print(f\"‚úÖ Saved updated triplets to {out_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "MfZMfNf5eXru",
        "outputId": "a570a349-03a6-4d25-a2cb-5d974078f34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'CSS3_HEX_TO_NAMES' from 'webcolors' (/usr/local/lib/python3.11/dist-packages/webcolors/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1bd6c641916b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwebcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCSS3_HEX_TO_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhex_to_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb_to_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclosest_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequested_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CSS3_HEX_TO_NAMES' from 'webcolors' (/usr/local/lib/python3.11/dist-packages/webcolors/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import webcolors\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Install webcolors if not installed\n",
        "# !pip install -q webcolors\n",
        "\n",
        "# Helper functions for color processing\n",
        "def extract_dominant_color(image_path, k=4):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize((100, 100))  # Resize for speed\n",
        "        image_np = np.array(image).reshape((-1, 3))\n",
        "\n",
        "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "        kmeans.fit(image_np)\n",
        "\n",
        "        # Find the most common cluster\n",
        "        counts = Counter(kmeans.labels_)\n",
        "        dominant_cluster = counts.most_common(1)[0][0]\n",
        "        dominant_color = kmeans.cluster_centers_[dominant_cluster].astype(int)\n",
        "        # Convert numpy integers to regular Python integers for JSON serialization\n",
        "        return tuple(int(x) for x in dominant_color)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def closest_color(requested_color):\n",
        "    \"\"\"Find the closest CSS3 color name for an RGB tuple\"\"\"\n",
        "    min_colors = {}\n",
        "\n",
        "    # Define basic CSS3 colors manually since webcolors API varies\n",
        "    css3_colors = {\n",
        "        'aliceblue': '#f0f8ff',\n",
        "        'antiquewhite': '#faebd7',\n",
        "        'aqua': '#00ffff',\n",
        "        'aquamarine': '#7fffd4',\n",
        "        'azure': '#f0ffff',\n",
        "        'beige': '#f5f5dc',\n",
        "        'bisque': '#ffe4c4',\n",
        "        'black': '#000000',\n",
        "        'blanchedalmond': '#ffebcd',\n",
        "        'blue': '#0000ff',\n",
        "        'blueviolet': '#8a2be2',\n",
        "        'brown': '#a52a2a',\n",
        "        'burlywood': '#deb887',\n",
        "        'cadetblue': '#5f9ea0',\n",
        "        'chartreuse': '#7fff00',\n",
        "        'chocolate': '#d2691e',\n",
        "        'coral': '#ff7f50',\n",
        "        'cornflowerblue': '#6495ed',\n",
        "        'cornsilk': '#fff8dc',\n",
        "        'crimson': '#dc143c',\n",
        "        'cyan': '#00ffff',\n",
        "        'darkblue': '#00008b',\n",
        "        'darkcyan': '#008b8b',\n",
        "        'darkgoldenrod': '#b8860b',\n",
        "        'darkgray': '#a9a9a9',\n",
        "        'darkgreen': '#006400',\n",
        "        'darkkhaki': '#bdb76b',\n",
        "        'darkmagenta': '#8b008b',\n",
        "        'darkolivegreen': '#556b2f',\n",
        "        'darkorange': '#ff8c00',\n",
        "        'darkorchid': '#9932cc',\n",
        "        'darkred': '#8b0000',\n",
        "        'darksalmon': '#e9967a',\n",
        "        'darkseagreen': '#8fbc8f',\n",
        "        'darkslateblue': '#483d8b',\n",
        "        'darkslategray': '#2f4f4f',\n",
        "        'darkturquoise': '#00ced1',\n",
        "        'darkviolet': '#9400d3',\n",
        "        'deeppink': '#ff1493',\n",
        "        'deepskyblue': '#00bfff',\n",
        "        'dimgray': '#696969',\n",
        "        'dodgerblue': '#1e90ff',\n",
        "        'firebrick': '#b22222',\n",
        "        'floralwhite': '#fffaf0',\n",
        "        'forestgreen': '#228b22',\n",
        "        'fuchsia': '#ff00ff',\n",
        "        'gainsboro': '#dcdcdc',\n",
        "        'ghostwhite': '#f8f8ff',\n",
        "        'gold': '#ffd700',\n",
        "        'goldenrod': '#daa520',\n",
        "        'gray': '#808080',\n",
        "        'green': '#008000',\n",
        "        'greenyellow': '#adff2f',\n",
        "        'honeydew': '#f0fff0',\n",
        "        'hotpink': '#ff69b4',\n",
        "        'indianred': '#cd5c5c',\n",
        "        'indigo': '#4b0082',\n",
        "        'ivory': '#fffff0',\n",
        "        'khaki': '#f0e68c',\n",
        "        'lavender': '#e6e6fa',\n",
        "        'lavenderblush': '#fff0f5',\n",
        "        'lawngreen': '#7cfc00',\n",
        "        'lemonchiffon': '#fffacd',\n",
        "        'lightblue': '#add8e6',\n",
        "        'lightcoral': '#f08080',\n",
        "        'lightcyan': '#e0ffff',\n",
        "        'lightgoldenrodyellow': '#fafad2',\n",
        "        'lightgray': '#d3d3d3',\n",
        "        'lightgreen': '#90ee90',\n",
        "        'lightpink': '#ffb6c1',\n",
        "        'lightsalmon': '#ffa07a',\n",
        "        'lightseagreen': '#20b2aa',\n",
        "        'lightskyblue': '#87cefa',\n",
        "        'lightslategray': '#778899',\n",
        "        'lightsteelblue': '#b0c4de',\n",
        "        'lightyellow': '#ffffe0',\n",
        "        'lime': '#00ff00',\n",
        "        'limegreen': '#32cd32',\n",
        "        'linen': '#faf0e6',\n",
        "        'magenta': '#ff00ff',\n",
        "        'maroon': '#800000',\n",
        "        'mediumaquamarine': '#66cdaa',\n",
        "        'mediumblue': '#0000cd',\n",
        "        'mediumorchid': '#ba55d3',\n",
        "        'mediumpurple': '#9370db',\n",
        "        'mediumseagreen': '#3cb371',\n",
        "        'mediumslateblue': '#7b68ee',\n",
        "        'mediumspringgreen': '#00fa9a',\n",
        "        'mediumturquoise': '#48d1cc',\n",
        "        'mediumvioletred': '#c71585',\n",
        "        'midnightblue': '#191970',\n",
        "        'mintcream': '#f5fffa',\n",
        "        'mistyrose': '#ffe4e1',\n",
        "        'moccasin': '#ffe4b5',\n",
        "        'navajowhite': '#ffdead',\n",
        "        'navy': '#000080',\n",
        "        'oldlace': '#fdf5e6',\n",
        "        'olive': '#808000',\n",
        "        'olivedrab': '#6b8e23',\n",
        "        'orange': '#ffa500',\n",
        "        'orangered': '#ff4500',\n",
        "        'orchid': '#da70d6',\n",
        "        'palegoldenrod': '#eee8aa',\n",
        "        'palegreen': '#98fb98',\n",
        "        'paleturquoise': '#afeeee',\n",
        "        'palevioletred': '#db7093',\n",
        "        'papayawhip': '#ffefd5',\n",
        "        'peachpuff': '#ffdab9',\n",
        "        'peru': '#cd853f',\n",
        "        'pink': '#ffc0cb',\n",
        "        'plum': '#dda0dd',\n",
        "        'powderblue': '#b0e0e6',\n",
        "        'purple': '#800080',\n",
        "        'red': '#ff0000',\n",
        "        'rosybrown': '#bc8f8f',\n",
        "        'royalblue': '#4169e1',\n",
        "        'saddlebrown': '#8b4513',\n",
        "        'salmon': '#fa8072',\n",
        "        'sandybrown': '#f4a460',\n",
        "        'seagreen': '#2e8b57',\n",
        "        'seashell': '#fff5ee',\n",
        "        'sienna': '#a0522d',\n",
        "        'silver': '#c0c0c0',\n",
        "        'skyblue': '#87ceeb',\n",
        "        'slateblue': '#6a5acd',\n",
        "        'slategray': '#708090',\n",
        "        'snow': '#fffafa',\n",
        "        'springgreen': '#00ff7f',\n",
        "        'steelblue': '#4682b4',\n",
        "        'tan': '#d2b48c',\n",
        "        'teal': '#008080',\n",
        "        'thistle': '#d8bfd8',\n",
        "        'tomato': '#ff6347',\n",
        "        'turquoise': '#40e0d0',\n",
        "        'violet': '#ee82ee',\n",
        "        'wheat': '#f5deb3',\n",
        "        'white': '#ffffff',\n",
        "        'whitesmoke': '#f5f5f5',\n",
        "        'yellow': '#ffff00',\n",
        "        'yellowgreen': '#9acd32'\n",
        "    }\n",
        "\n",
        "    for name, hex_value in css3_colors.items():\n",
        "        # Convert hex to RGB\n",
        "        r_c, g_c, b_c = webcolors.hex_to_rgb(hex_value)\n",
        "\n",
        "        # Calculate Euclidean distance\n",
        "        rd = (r_c - requested_color[0]) ** 2\n",
        "        gd = (g_c - requested_color[1]) ** 2\n",
        "        bd = (b_c - requested_color[2]) ** 2\n",
        "        min_colors[(rd + gd + bd)] = name\n",
        "\n",
        "    return min_colors[min(min_colors.keys())]\n",
        "\n",
        "def rgb_to_color_name(rgb_tuple):\n",
        "    \"\"\"Convert RGB tuple to the closest CSS3 color name\"\"\"\n",
        "    if rgb_tuple is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Try to get exact match first\n",
        "        return webcolors.rgb_to_name(rgb_tuple)\n",
        "    except ValueError:\n",
        "        # If no exact match, find closest color\n",
        "        return closest_color(rgb_tuple)\n",
        "\n",
        "# Helper function to find image file with different extensions\n",
        "def find_image_file(base_path, image_id):\n",
        "    \"\"\"Try to find image file with different extensions\"\"\"\n",
        "    extensions = ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']\n",
        "    for ext in extensions:\n",
        "        image_path = base_path / f\"{image_id}{ext}\"\n",
        "        if image_path.exists():\n",
        "            return image_path\n",
        "    return None\n",
        "\n",
        "# === Paths and setup ===\n",
        "base_path = Path(\"/content/fashion-iq\")\n",
        "cleaned_images_dir = Path(\"/content/fashion-iq-cleaned\")\n",
        "captions_dir = base_path / \"captions\"\n",
        "output_dir = base_path / \"captions\" / \"captions_with_colors2\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "dress_types = ['dress', 'shirt', 'toptee']\n",
        "splits = ['train', 'val', 'test']  # or just ['train'] if needed\n",
        "\n",
        "# === Loop through all caption files ===\n",
        "for dress_type in dress_types:\n",
        "    for split in splits:\n",
        "        caption_file = captions_dir / f\"cap.{dress_type}.{split}.json\"\n",
        "        if not caption_file.exists():\n",
        "            print(f\"‚ùå Skipping missing file: {caption_file}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"üé® Processing {caption_file}...\")\n",
        "        with open(caption_file, 'r') as f:\n",
        "            triplets = json.load(f)\n",
        "\n",
        "        processed_count = 0\n",
        "        missing_images = []\n",
        "\n",
        "        for triplet in tqdm(triplets):\n",
        "            # Try to find candidate image with different extensions\n",
        "            candidate_path = find_image_file(cleaned_images_dir, triplet['candidate'])\n",
        "            target_path = find_image_file(cleaned_images_dir, triplet['target'])\n",
        "\n",
        "            # Process candidate image\n",
        "            if candidate_path:\n",
        "                candidate_color = extract_dominant_color(candidate_path)\n",
        "                triplet['candidate_color_rgb'] = candidate_color\n",
        "                triplet['candidate_color_name'] = rgb_to_color_name(candidate_color)\n",
        "            else:\n",
        "                missing_images.append(f\"{triplet['candidate']}\")\n",
        "                triplet['candidate_color_rgb'] = None\n",
        "                triplet['candidate_color_name'] = None\n",
        "\n",
        "            # Process target image\n",
        "            if target_path:\n",
        "                target_color = extract_dominant_color(target_path)\n",
        "                triplet['target_color_rgb'] = target_color\n",
        "                triplet['target_color_name'] = rgb_to_color_name(target_color)\n",
        "            else:\n",
        "                missing_images.append(f\"{triplet['target']}\")\n",
        "                triplet['target_color_rgb'] = None\n",
        "                triplet['target_color_name'] = None\n",
        "\n",
        "            if candidate_path or target_path:\n",
        "                processed_count += 1\n",
        "\n",
        "        # Save updated JSON\n",
        "        out_file = output_dir / f\"cap.{dress_type}.{split}.json\"\n",
        "        with open(out_file, 'w') as f:\n",
        "            json.dump(triplets, f, indent=2)\n",
        "\n",
        "        print(f\"‚úÖ Saved updated triplets to {out_file}\")\n",
        "        print(f\"üìä Processed: {processed_count}/{len(triplets)} triplets\")\n",
        "        if missing_images:\n",
        "            print(f\"‚ö†Ô∏è  Missing {len(set(missing_images))} unique images\")\n",
        "            # Optionally save missing images list\n",
        "            missing_file = output_dir / f\"missing_images_{dress_type}_{split}.txt\"\n",
        "            with open(missing_file, 'w') as f:\n",
        "                f.write('\\n'.join(sorted(set(missing_images))))\n",
        "            print(f\"üìù Missing images list saved to {missing_file}\")\n",
        "\n",
        "print(\"üéâ Color processing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "yGbgnY66lty_",
        "outputId": "88af3597-0c0e-4be0-a1ef-840b6f8a2c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé® Processing /content/fashion-iq/captions/cap.dress.train.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5985/5985 [02:12<00:00, 45.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved updated triplets to /content/fashion-iq/captions/captions_with_colors2/cap.dress.train.json\n",
            "üìä Processed: 5985/5985 triplets\n",
            "‚ö†Ô∏è  Missing 41 unique images\n",
            "üìù Missing images list saved to /content/fashion-iq/captions/captions_with_colors2/missing_images_dress_train.txt\n",
            "üé® Processing /content/fashion-iq/captions/cap.dress.val.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2017/2017 [00:44<00:00, 45.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved updated triplets to /content/fashion-iq/captions/captions_with_colors2/cap.dress.val.json\n",
            "üìä Processed: 2016/2017 triplets\n",
            "‚ö†Ô∏è  Missing 19 unique images\n",
            "üìù Missing images list saved to /content/fashion-iq/captions/captions_with_colors2/missing_images_dress_val.txt\n",
            "üé® Processing /content/fashion-iq/captions/cap.dress.test.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2024 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'target'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f87418e06927>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Try to find candidate image with different extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mcandidate_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_images_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_images_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# Process candidate image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'target'"
          ]
        }
      ]
    }
  ]
}