{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQFWnv0RPAx_",
        "outputId": "d4576c10-a709-4a29-b469-5f075b0481fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZEI5zjwQ8jE",
        "outputId": "0689ed3e-f9af-4d3c-a346-4c3af7fc413c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Copied image_splits to /content/fashion-iq/image_splits\n",
            "✅ Copied captions to /content/fashion-iq/captions\n",
            "replace /content/fashion-iq/images/images/245600258X.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: ✅ Unzipped images.zip into Colab storage.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "drive_base = '/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq'\n",
        "colab_base = '/content/fashion-iq'\n",
        "\n",
        "# Create destination root in Colab internal storage\n",
        "os.makedirs(colab_base, exist_ok=True)\n",
        "\n",
        "# Define subdirectories\n",
        "subdirs = ['image_splits', 'captions']\n",
        "\n",
        "# Copy 'image_splits' and 'captions' directories\n",
        "for subdir in subdirs:\n",
        "    src = os.path.join(drive_base, subdir)\n",
        "    dst = os.path.join(colab_base, subdir)\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)  # Remove existing to avoid duplication/conflict\n",
        "    shutil.copytree(src, dst)\n",
        "    print(f\"✅ Copied {subdir} to {dst}\")\n",
        "\n",
        "# Handle images.zip\n",
        "zip_path = os.path.join(drive_base, 'images/images.zip')\n",
        "extract_dir = os.path.join(colab_base, 'images')\n",
        "\n",
        "# Make sure destination for images exists\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip images.zip into /content/fashion-iq/images/\n",
        "!unzip -q \"$zip_path\" -d \"$extract_dir\"\n",
        "\n",
        "print(\"✅ Unzipped images.zip into Colab storage.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VJUbUdoRLMz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "drive_base = '/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq'\n",
        "colab_base = '/content/fashion-iq'\n",
        "base_path =  Path(\"/content/fashion-iq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCYoUahbRUdq",
        "outputId": "99f84b92-b8b8-4bba-c970-5d819069373e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQfrIYE9RiOT",
        "outputId": "fda8ae61-77e5-4a68-b1dc-a9555670b0dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Copied image_splits to /content/fashion-iq/image_splits\n",
            "✅ Copied captions to /content/fashion-iq/captions\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/image_retrieval')\n",
        "import importlib\n",
        "import data_utils  # initial import\n",
        "from data_utils import targetpad_transform,_convert_image_to_rgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95KbjM6wTSq9",
        "outputId": "aa3f8f9e-46ba-4e20-fdf9-bf93bf4df764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-io4dn95y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-io4dn95y\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FwarHKkTWPF"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHTIkImLT0F0"
      },
      "outputs": [],
      "source": [
        "transform = targetpad_transform(target_ratio=1.0, dim=224)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W5H3SknT-3b"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"a clothing item\",\n",
        "    \"a fashion dress\",\n",
        "    \"a shirt or top\",\n",
        "    \"a brand logo\",\n",
        "    \"an image with only text\",\n",
        "    \"an empty product image\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bviZ9vG4UB7y"
      },
      "outputs": [],
      "source": [
        "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
        "text_tokens = clip.tokenize(prompts).to(device)\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aubFwWgTUj2C"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnbeyplaUmZo"
      },
      "outputs": [],
      "source": [
        "image_folder = base_path / 'images' / 'images'\n",
        "output_folder = Path(\"/content/fashion-iq-cleaned\")\n",
        "output_folder.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-1mA8RJUowh",
        "outputId": "ef884564-f59f-4ad7-d19d-09c9dd3209a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 74381/74381 [17:29<00:00, 70.88it/s]\n"
          ]
        }
      ],
      "source": [
        "unrelated_images = []\n",
        "for img_path in tqdm(list(image_folder.glob(\"*.png\"))):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_features = model.encode_image(img_tensor)\n",
        "            img_features /= img_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
        "            best_prompt_idx = similarity.argmax().item()\n",
        "            best_prompt = prompts[best_prompt_idx]\n",
        "\n",
        "            if best_prompt in [\"a brand logo\", \"an image with only text\", \"an empty product image\"]:\n",
        "                unrelated_images.append(str(img_path))\n",
        "            else:\n",
        "                shutil.copy(img_path, output_folder / img_path.name)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading {img_path.name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcTUYPGKcrQh",
        "outputId": "f15b91d0-f52b-4198-c8cd-b6a93762ad25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cleaned dataset successfully saved to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "cleaned_colab_path = \"/content/fashion-iq-cleaned\"\n",
        "drive_destination = \"/content/drive/MyDrive/Colab Notebooks/image_retrieval/dataset/fashion-iq-cleaned\"\n",
        "\n",
        "# Remove if already exists in Drive to avoid duplication (optional)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(drive_destination):\n",
        "    shutil.rmtree(drive_destination)\n",
        "\n",
        "# Copy the cleaned folder to your Drive\n",
        "shutil.copytree(cleaned_colab_path, drive_destination)\n",
        "\n",
        "print(\"✅ Cleaned dataset successfully saved to Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XK2UCTCd_D9",
        "outputId": "3593e08c-4074-4678-9861-1508731bf532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Number of images in '/content/fashion-iq-cleaned': 73620\n"
          ]
        }
      ],
      "source": [
        "cleaned_path = \"/content/fashion-iq-cleaned\"\n",
        "\n",
        "# Count image files (e.g., .png, .jpg, .jpeg)\n",
        "image_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
        "image_count = sum(\n",
        "    1 for fname in os.listdir(cleaned_path) if fname.lower().endswith(image_extensions)\n",
        ")\n",
        "\n",
        "print(f\"✅ Number of images in '{cleaned_path}': {image_count}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
