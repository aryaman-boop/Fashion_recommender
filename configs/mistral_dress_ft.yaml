base_model: mistralai/Mistral-7B-Instruct-v0.1
output_dir: ./lora-out-dress-checkpoint-single

adapter: lora
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj

dataset:
  path: data/json_files/instruction_dataset.jsonl
  type: alpaca
  field: text
  train_on_split: train

sequence_len: 512
sample_packing: true
pad_to_sequence_len: true

val_set_size: 0.05
output_dir: lora-out-dress-checkpoint-single

batch_size: 1
micro_batch_size: 1     # <- single GPU setting
num_epochs: 3
learning_rate: 2e-5

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
warmup_steps: 10

gradient_checkpointing: true
gradient_accumulation_steps: 4

use_wandb: true
wandb_project: dress-finetune-single-gpu

mixed_precision: bf16
log_dir: logs